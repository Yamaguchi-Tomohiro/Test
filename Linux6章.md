## Translation Lokkaside Buffer
プロセスが仮想アドレスのデータにアクセスするには次の手順が必要である。
1 物理メモリ上に存在するページテーブルを参照し、仮想アドレスと物理アドレスを変換
2 1で求めた物理アドレスへのアクセス

前回の学習内容であるキャッシュメモリによって高速化できるのは手順2のみである。
手順1のページテーブルは物理メモリ上に存在するため、これにアクセスする必要がある。
この場合、キャッシュメモリの恩恵はなくなってしまう。

この問題を解決するため、CPUでは仮想アドレスから物理アドレスへの変換表を保持する。
キャッシュメモリと同様に高速にアクセス可能な、Translation Lokkaside Bufferという領域によって、①の手順も高速化できる。

本では詳しく言及していないが、MMUの中の一つで、直近のアドレス変換を保持するキャッシュである。
参考URL：https://www.ipa.go.jp/files/000056289.pdf


## ページキャッシュ

前回学習した、CPUからメモリへのアクセス速度とストレージへのアクセスの速度の差を埋めるための仕組みが、「ページキャッシュ」である。

ページキャッシュでは、ストレージデバイス上のファイルデータをメモリにキャッシュする。
また、名の通りページ単位でキャッシュしたデータを取扱う。

### ページキャッシュの読み込みを高速化する流れ

1. カーネルのメモリ上に存在する、ページキャッシュという領域にファイル(ストレージデバイス上)のデータをコピーする
2. 1でコピーしたデータをプロセスのメモリにコピーするして使用する
3. ページキャッシュ上に存在するデータが再び読みだされた場合、ページキャッシュに存在するデータを返す
<br> ※ページキャッシュはプロセス全体の資源であるため、どのプロセスからでも呼び出すことが可能

### ページキャッシュの書き込みの流れ

1. プロセスがデータをファイルに書き込むと、カーネルはページキャッシュだけにデータを書き込む
<br> この時、該当データに「データの内容は、ストレージデバイス上のものより新しい」という印をつける。　このページをダーティページと呼ぶ

2. カーネルのバックグラウンド処理によって、ストレージ内のファイル反映される
<br> この時、ダーティページという印も消す

上記の2通りの処理では、プロセスが必要とする毎にストレージデバイスにアクセスする場合に比べて、処理は早く終わる。
各プロセスによってアクセスされるファイル上のデータが、すべてページキャッシュ上に存在する場合、システムのファイルアクセス速度は、メモリアクセス速度に近くなる。
よって、システム全体の大幅な高速化が期待できる。

ページキャッシュのサイズは、システムのメモリに空きがある限りシステム内の各プロセスがページキャッシュ上にまだないファイルにアクセスする毎に増加する
システムのメモリが足りなくなれば、カーネルがページキャッシュを開放し、空き領域を用意する

この場合、ダーティでないページを廃棄する
それでも足りない場合、ダーティページをライトバックした後に廃棄する
後者の場合、ストレージアクセスが発生するため、システムの性能が劣化する恐れがある
